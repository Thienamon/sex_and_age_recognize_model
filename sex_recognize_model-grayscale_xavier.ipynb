{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_file_name = 'face_data.npz'\n",
    "test_file_name = 'test_data.npz'\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def init_weights(shape, name):\n",
    "    \"\"\"\n",
    "    initialize weight\n",
    "    shape: shape of weight\n",
    "    name: weight name\n",
    "    xavier 초기화 사용\n",
    "    각각의 Filter를 만들어주는 함수\n",
    "    \"\"\"\n",
    "    with tf.device('/gpu:0'):\n",
    "        return tf.get_variable(name = name, shape = shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def init_bias(shape, name):\n",
    "    \"\"\"\n",
    "    initialize bias\n",
    "    bias를 만들어주는 함수\n",
    "    \"\"\"\n",
    "    with tf.device('/gpu:0'):\n",
    "        return tf.Variable(tf.constant(0.1, shape = shape))\n",
    "\n",
    "def model(X, w1, w2, w3, w4, wo, b2, b3, b4, bo, p_keep_conv, p_keep_hidden):\n",
    "    \"\"\"\n",
    "    create model\n",
    "    X: first input\n",
    "    w1, w2, w3, wo: weights for each layer\n",
    "    b2, b3, bo: bias for each layer\n",
    "    p_keep_conv, p_keep_hidden: constant for dropout\n",
    "    \n",
    "    모델생성 함수.\n",
    "    총 3개의 Conv layers와 3개의 Max-pool layers, 2개의 FC layers로 구성\n",
    "    Conv1 - Pooling1 - Conv2 - Pooling2 - Conv3 - Pooling3 - FC1 - FC2\n",
    "    로 구성되어있고 각 풀링 이후와 첫번째 FC 이후에 dropout을 적용\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "#         dw = tf.nn.max_pool(X, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') \n",
    "\n",
    "        # First Conv layer. l1a shape = (?, 128, 128, 32)\n",
    "        with tf.name_scope(\"layer1\"):\n",
    "            l1a = tf.nn.relu(tf.nn.conv2d(X, w1, strides = [1, 1, 1, 1], padding = 'SAME'))\n",
    "            # First Max-pool layer. l1 shape = (?, 64, 64, 32)\n",
    "            l1 = tf.nn.max_pool(l1a, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')        \n",
    "            l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "\n",
    "        with tf.name_scope(\"layer2\"):\n",
    "            # l2a shape = (?, 64, 64, 64)\n",
    "            l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, strides = [1, 1, 1, 1], padding = 'SAME') + b2)\n",
    "            # l2 shape = (?, 32, 32, 64)\n",
    "            l2 = tf.nn.max_pool(l2a, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "            l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "\n",
    "        with tf.name_scope(\"layer3\"):\n",
    "            # l2a shape = (?, 64, 64, 64)\n",
    "            l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, strides = [1, 1, 1, 1], padding = 'SAME') + b3)\n",
    "            # l2 shape = (?, 32, 32, 64)\n",
    "            l3 = tf.nn.max_pool(l3a, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "            print(l3.shape)\n",
    "            l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])\n",
    "            l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "\n",
    "        with tf.name_scope(\"layer4\"):\n",
    "            # reshape to ( , 3072)\n",
    "            l4 = tf.nn.relu(tf.matmul(l3, w4) + b4)\n",
    "            l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "            l4 = tf.matmul(l4, wo) + bo\n",
    "            return l4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load success\n",
      "(1785, 128, 128, 1)\n",
      "(166, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어옴\n",
    "if os.path.isfile(data_file_name):\n",
    "    file = np.load(data_file_name)\n",
    "    data = file['data']\n",
    "    labels = file['sex_labels']\n",
    "else:\n",
    "    print(\"No data file.\")\n",
    "    sys.exit()\n",
    "    \n",
    "if os.path.isfile(test_file_name):\n",
    "    file = np.load(test_file_name)\n",
    "    td = file['data']\n",
    "    tl = file['sex_labels']\n",
    "else:\n",
    "    print(\"No test file\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"data load success\")\n",
    "\n",
    "print(data.shape)\n",
    "print(td.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 128)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# data, labels 를 위한 placeholder\n",
    "X = tf.placeholder(\"float\", [None, 128, 128, 1], name = \"X\")\n",
    "Y = tf.placeholder(\"float\", [None, 2], name = \"Y\")\n",
    "\n",
    "# data, labels 를 위한 placeholder\n",
    "w1 = init_weights([3, 3, 1, 32], \"weight1\") \n",
    "w2 = init_weights([3, 3, 32, 64], \"weight2\")\n",
    "w3 = init_weights([3, 3, 64, 128], \"weight3\")\n",
    "w4 = init_weights([16*16*128, 512], \"weight4\")\n",
    "wo = init_weights([512, 2], \"weight_out\")\n",
    "\n",
    "b2 = init_bias([64], \"bias2\")\n",
    "b3 = init_bias([128], \"bias3\")\n",
    "b4 = init_bias([512], \"bias4\")\n",
    "bo = init_bias([2], \"bias_out\")\n",
    "\n",
    "# tensorboard에 weights와 bias histogram 등록\n",
    "tf.summary.histogram(\"weight1_summ\", w1)\n",
    "tf.summary.histogram(\"weight2_summ\", w2)\n",
    "tf.summary.histogram(\"bias2_summ\", b2)\n",
    "tf.summary.histogram(\"weight3_summ\", w3)\n",
    "tf.summary.histogram(\"bias3_summ\", b3)\n",
    "tf.summary.histogram(\"weight4_summ\", w4)\n",
    "tf.summary.histogram(\"bias4_summ\", b4)\n",
    "tf.summary.histogram(\"weight_out_summ\", wo)\n",
    "tf.summary.histogram(\"bias_out_summ\", bo)\n",
    "\n",
    "# dropout 위한 placeholder\n",
    "p_keep_conv = tf.placeholder(\"float\", name = \"p_keep_conv\")\n",
    "p_keep_hidden = tf.placeholder(\"float\", name = \"p_keep_hidden\")\n",
    "\n",
    "# 모델생성\n",
    "M = model(X, w1, w2, w3, w4, wo, b2, b3, b4, bo, p_keep_conv, p_keep_hidden)\n",
    "\n",
    "# cost와 accuarcy를 계산해주는 함수 설정\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = M, labels = Y))\n",
    "    train_op = tf.train.RMSPropOptimizer(0.00001, 0.9).minimize(cost)\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"accuarcy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(M, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "    tf.summary.scalar(\"accuracy\", acc_op)\n",
    "\n",
    "# 모델 저장을 위해 collection에 weight와 bias모음\n",
    "tf.add_to_collection('vars', w1)\n",
    "tf.add_to_collection('vars', w2)\n",
    "tf.add_to_collection('vars', b2)\n",
    "tf.add_to_collection('vars', w3)\n",
    "tf.add_to_collection('vars', b3)\n",
    "tf.add_to_collection('vars', w4)\n",
    "tf.add_to_collection('vars', b4)\n",
    "tf.add_to_collection('vars', wo)\n",
    "tf.add_to_collection('vars', bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:38.691027\n",
      "0 13.2721449309 0.0783132\n",
      "1 10.7473105593 0.0783133\n",
      "2 7.49517394488 0.0843374\n",
      "3 7.81770165207 0.0903614\n",
      "4 5.93124557815 0.114458\n",
      "5 6.08763130188 0.114458\n",
      "6 5.66423679244 0.108434\n",
      "7 4.55608227426 0.168675\n",
      "8 5.21829625255 0.138554\n",
      "9 4.64874382088 0.186747\n",
      "10 3.52650795534 0.144578\n",
      "11 3.77380854773 0.192771\n",
      "12 3.74556110885 0.168675\n",
      "13 3.07030856005 0.240964\n",
      "14 2.96178135984 0.138554\n",
      "15 2.1877068913 0.204819\n",
      "16 1.94038394735 0.23494\n",
      "17 2.07050676071 0.246988\n",
      "18 1.69376654035 0.307229\n",
      "19 1.64356003726 0.331325\n",
      "20 1.23915267234 0.337349\n",
      "21 1.2817294958 0.343374\n",
      "22 1.06248817939 0.301205\n",
      "23 1.05134177545 0.307229\n",
      "24 0.882981113158 0.307229\n",
      "25 0.819159989174 0.337349\n",
      "26 0.743797441622 0.295181\n",
      "27 0.679409009906 0.331325\n",
      "28 0.695551010565 0.349398\n",
      "29 0.751563520386 0.463855\n",
      "30 0.64924338331 0.385542\n",
      "31 0.739188133094 0.457831\n",
      "32 0.62272910043 0.421687\n",
      "33 0.56154904142 0.445783\n",
      "34 0.5992943901 0.385542\n",
      "35 0.566369408885 0.493976\n",
      "36 0.603624911262 0.403614\n",
      "37 0.60451424609 0.481928\n",
      "38 0.668716406593 0.524096\n",
      "39 0.595618486978 0.463855\n",
      "40 0.655284647185 0.475904\n",
      "41 0.628275587009 0.512048\n",
      "42 0.583021516697 0.506024\n",
      "43 0.63617816768 0.506024\n",
      "44 0.565808489059 0.524096\n",
      "45 0.544161894574 0.572289\n",
      "46 0.582025805345 0.524096\n",
      "47 0.546410765212 0.554217\n",
      "48 0.585400524621 0.536145\n",
      "49 0.541844155353 0.487952\n",
      "50 0.517680406284 0.536144\n",
      "51 0.573785290265 0.542169\n",
      "52 0.552725557811 0.536145\n",
      "53 0.552585193744 0.548193\n",
      "54 0.517593504718 0.566265\n",
      "55 0.543917715836 0.554217\n",
      "56 0.532656531781 0.554217\n",
      "57 0.502944125579 0.5\n",
      "58 0.518107637763 0.542169\n",
      "59 0.640214941536 0.554217\n",
      "60 0.522881197242 0.578313\n",
      "61 0.511347444012 0.63253\n",
      "62 0.597393667755 0.590361\n",
      "63 0.554313096003 0.584337\n",
      "64 0.519181589954 0.566265\n",
      "65 0.532521116905 0.560241\n",
      "66 0.510107537302 0.572289\n",
      "67 0.500038172906 0.578313\n",
      "68 0.553264379788 0.590361\n",
      "69 0.538167860932 0.608434\n",
      "70 0.577764948543 0.608434\n",
      "71 0.52272959684 0.608434\n",
      "72 0.500847583207 0.590361\n",
      "73 0.544013283287 0.644578\n",
      "74 0.51459957344 0.60241\n",
      "75 0.491212761746 0.578313\n",
      "76 0.535473701472 0.524096\n",
      "77 0.500520874818 0.596386\n",
      "78 0.498224366743 0.590361\n",
      "79 0.477758404154 0.578313\n",
      "80 0.477208482818 0.554217\n",
      "81 0.469386236026 0.578313\n",
      "82 0.500203743004 0.554217\n",
      "83 0.492040119492 0.518072\n",
      "84 0.467737476413 0.554217\n",
      "85 0.464218850892 0.566265\n",
      "86 0.465823620558 0.566265\n",
      "87 0.52380058198 0.554217\n",
      "88 0.525430282435 0.518072\n",
      "89 0.485349885116 0.493976\n",
      "90 0.466991529299 0.512048\n",
      "91 0.474134897956 0.548193\n",
      "92 0.472403048896 0.578313\n",
      "93 0.463276841893 0.578313\n",
      "94 0.443856143321 0.578313\n",
      "95 0.501686688226 0.566265\n",
      "96 0.503690005495 0.572289\n",
      "97 0.469869231948 0.548193\n",
      "98 0.467302619265 0.5\n",
      "99 0.513675663047 0.554217\n",
      "100 0.472366778323 0.572289\n",
      "101 0.464676854416 0.572289\n",
      "102 0.459167206803 0.590361\n",
      "103 0.455384221215 0.584337\n",
      "104 0.479814262058 0.572289\n",
      "105 0.453644825862 0.578313\n",
      "106 0.456406685882 0.584337\n",
      "107 0.441066793238 0.554217\n",
      "108 0.471620363112 0.536145\n",
      "109 0.435197022099 0.542169\n",
      "110 0.462972805477 0.53012\n",
      "111 0.45193127686 0.572289\n",
      "112 0.476720514636 0.566265\n",
      "113 0.465177375537 0.536145\n",
      "114 0.479789489021 0.566265\n",
      "115 0.44704765325 0.560241\n",
      "116 0.487192626183 0.590361\n",
      "117 0.461749089165 0.614458\n",
      "118 0.42008342422 0.566265\n",
      "119 0.437810492773 0.590361\n",
      "120 0.409893283764 0.572289\n",
      "121 0.415244515126 0.560241\n",
      "122 0.421166854696 0.566265\n",
      "123 0.422649308179 0.590361\n",
      "124 0.423880049529 0.620482\n",
      "125 0.408861955485 0.63253\n",
      "126 0.425035221932 0.596386\n",
      "127 0.406751349282 0.620482\n",
      "128 0.402207839374 0.572289\n",
      "129 0.41168108124 0.578313\n",
      "130 0.401206736954 0.60241\n",
      "131 0.40050695464 0.590361\n",
      "132 0.419783458973 0.548193\n",
      "133 0.39416542506 0.554217\n",
      "134 0.405281348584 0.578313\n",
      "135 0.417498180499 0.614458\n",
      "136 0.380808035198 0.608434\n",
      "137 0.404028810274 0.650602\n",
      "138 0.415362560405 0.626506\n",
      "139 0.387343357532 0.614458\n",
      "140 0.445765337262 0.638554\n",
      "141 0.401591502273 0.638554\n",
      "142 0.37470712613 0.626506\n",
      "143 0.404082206436 0.63253\n",
      "144 0.411651885853 0.620482\n",
      "145 0.382170677687 0.590361\n",
      "146 0.363482242307 0.566265\n",
      "147 0.367946368427 0.60241\n",
      "148 0.410194442106 0.60241\n",
      "149 0.41871620772 0.620482\n",
      "150 0.379700560696 0.614458\n",
      "151 0.401962151321 0.590361\n",
      "152 0.38393763725 0.590361\n",
      "153 0.388735405241 0.620482\n",
      "154 0.408688170262 0.614458\n",
      "155 0.403517947174 0.620482\n",
      "156 0.406476740367 0.63253\n",
      "157 0.385621508727 0.626506\n",
      "158 0.403519435571 0.656626\n",
      "159 0.365125291909 0.63253\n",
      "160 0.350282372334 0.63253\n",
      "161 0.403420102281 0.638554\n",
      "162 0.379500940299 0.656626\n",
      "163 0.367749606474 0.620482\n",
      "164 0.365988621345 0.60241\n",
      "165 0.390616616807 0.60241\n",
      "166 0.401817306733 0.614458\n",
      "167 0.379969004828 0.620482\n",
      "168 0.384463637208 0.650602\n",
      "169 0.422563941433 0.626506\n",
      "170 0.393144465123 0.608434\n",
      "171 0.403523539694 0.608434\n",
      "172 0.362594379542 0.596386\n",
      "173 0.382251502803 0.60241\n",
      "174 0.37878296295 0.614458\n",
      "175 0.336665120979 0.554217\n",
      "176 0.333964821238 0.536145\n",
      "177 0.336494373731 0.590361\n",
      "178 0.379148881596 0.60241\n",
      "179 0.38227424484 0.608434\n",
      "180 0.360576892415 0.596386\n",
      "181 0.349255734338 0.638554\n",
      "182 0.344492413605 0.63253\n",
      "183 0.328151509882 0.620482\n",
      "184 0.354177392876 0.63253\n",
      "185 0.356138594019 0.614458\n",
      "186 0.333934804854 0.596386\n",
      "187 0.359990846079 0.536145\n",
      "188 0.364625676893 0.542169\n",
      "189 0.363262635154 0.578313\n",
      "190 0.342270679772 0.572289\n",
      "191 0.369643021088 0.578313\n",
      "192 0.337603283043 0.566265\n",
      "193 0.35119829986 0.590361\n",
      "194 0.344819197145 0.60241\n",
      "195 0.341754765465 0.596385\n",
      "196 0.347038956502 0.590361\n",
      "197 0.340528292725 0.614458\n",
      "198 0.333530779641 0.578313\n",
      "199 0.364238317196 0.572289\n",
      "200 0.370728740039 0.572289\n",
      "201 0.358807246129 0.578313\n",
      "202 0.361331889119 0.566265\n",
      "203 0.334826780913 0.572289\n",
      "204 0.354518868172 0.60241\n",
      "205 0.350063397477 0.60241\n",
      "206 0.34829588349 0.614458\n",
      "207 0.329990498865 0.584337\n",
      "208 0.326166963635 0.590361\n",
      "209 0.354906520018 0.590361\n",
      "210 0.320929522411 0.584337\n",
      "211 0.330480325394 0.60241\n",
      "212 0.358216339006 0.668675\n",
      "213 0.37364659659 0.644578\n",
      "214 0.362295733478 0.644578\n",
      "215 0.367259723349 0.63253\n",
      "216 0.346016658613 0.608434\n",
      "217 0.341181101277 0.614458\n",
      "218 0.320617942713 0.620482\n",
      "219 0.319048301388 0.614458\n",
      "220 0.32477657196 0.650602\n",
      "221 0.313629759046 0.644578\n",
      "222 0.337463907468 0.638554\n",
      "223 0.355631843997 0.650602\n",
      "224 0.343080952907 0.638554\n",
      "225 0.310358413423 0.662651\n",
      "226 0.334329109424 0.674699\n",
      "227 0.347484558821 0.662651\n",
      "228 0.346608586036 0.626506\n",
      "229 0.347894448214 0.644578\n",
      "230 0.328792964825 0.650602\n",
      "231 0.304463591713 0.63253\n",
      "232 0.320417484555 0.638554\n",
      "233 0.300774169656 0.626506\n",
      "234 0.311569575793 0.63253\n",
      "235 0.355548982437 0.644578\n",
      "236 0.338569618188 0.626506\n",
      "237 0.338749432936 0.63253\n",
      "238 0.35059148589 0.620482\n",
      "239 0.299154385065 0.608434\n",
      "240 0.305469244289 0.608434\n",
      "241 0.302103637216 0.614458\n",
      "242 0.320753735705 0.608434\n",
      "243 0.313520288525 0.608434\n",
      "244 0.331159087901 0.614458\n",
      "245 0.306967407752 0.614458\n",
      "246 0.319940457264 0.620482\n",
      "247 0.320446912056 0.620482\n",
      "248 0.320023024311 0.650602\n",
      "249 0.347338101039 0.668675\n",
      "250 0.318342389682 0.638554\n",
      "251 0.329924842773 0.674699\n",
      "252 0.324068946907 0.656626\n",
      "253 0.30791777573 0.626506\n",
      "254 0.2948345605 0.63253\n",
      "255 0.291844193322 0.596386\n",
      "256 0.32382998988 0.626506\n",
      "257 0.302302678617 0.650602\n",
      "258 0.299216646415 0.638554\n",
      "259 0.2918370816 0.620482\n",
      "260 0.322936742065 0.614458\n",
      "261 0.321337348853 0.644578\n",
      "262 0.296841047561 0.626506\n",
      "263 0.294812007019 0.626506\n",
      "264 0.294401317118 0.63253\n",
      "265 0.33671802182 0.638554\n",
      "266 0.305586165534 0.650602\n",
      "267 0.309779242541 0.662651\n",
      "268 0.293818757798 0.686747\n",
      "269 0.301444301095 0.650602\n",
      "270 0.313012480163 0.650602\n",
      "271 0.30744711195 0.650602\n",
      "272 0.287711899226 0.620482\n",
      "273 0.295080999056 0.63253\n",
      "274 0.314762523255 0.644578\n",
      "275 0.271609507644 0.63253\n",
      "276 0.272197221191 0.63253\n",
      "277 0.300963948696 0.638554\n",
      "278 0.298593189281 0.626506\n",
      "279 0.284118942057 0.620482\n",
      "280 0.28261217217 0.638554\n",
      "281 0.311074479268 0.620482\n",
      "282 0.292155905412 0.614458\n",
      "283 0.29427110633 0.626506\n",
      "284 0.282835630843 0.620482\n",
      "285 0.294800008289 0.63253\n",
      "286 0.314321820767 0.644578\n",
      "287 0.29021607454 0.644578\n",
      "288 0.281085148024 0.692771\n",
      "289 0.29393182437 0.674699\n",
      "290 0.288935326613 0.698795\n",
      "291 0.278285867606 0.704819\n",
      "292 0.286650338712 0.686747\n",
      "293 0.291493327858 0.638554\n",
      "294 0.274814899868 0.650602\n",
      "295 0.268589996303 0.692771\n",
      "296 0.283267734572 0.692771\n",
      "297 0.277752006713 0.680723\n",
      "298 0.309884517525 0.674699\n",
      "299 0.288975587401 0.656626\n",
      "300 0.297966780284 0.656626\n",
      "301 0.271853333053 0.668675\n",
      "302 0.275793651119 0.656626\n",
      "303 0.273281257886 0.668675\n",
      "304 0.290580887562 0.662651\n",
      "305 0.269912380725 0.674699\n",
      "306 0.261072877795 0.674699\n",
      "307 0.269177294408 0.668675\n",
      "308 0.286122719829 0.656626\n",
      "309 0.266522902422 0.686747\n",
      "310 0.263356006489 0.686747\n",
      "311 0.272586337935 0.656626\n",
      "312 0.290110268487 0.662651\n",
      "313 0.281336160377 0.686747\n",
      "314 0.248570199196 0.668675\n",
      "315 0.272870689917 0.668675\n",
      "316 0.258721108907 0.674699\n",
      "317 0.25692740183 0.674699\n",
      "318 0.24806703971 0.668675\n",
      "319 0.274862184261 0.674699\n",
      "320 0.249563707469 0.680723\n",
      "321 0.291989790992 0.680723\n",
      "322 0.258427260873 0.692771\n",
      "323 0.261962143848 0.698795\n",
      "324 0.281536406909 0.698795\n",
      "325 0.264682743698 0.704819\n",
      "326 0.271358771823 0.656627\n",
      "327 0.280451455727 0.662651\n",
      "328 0.274368838049 0.644578\n",
      "329 0.26649180112 0.680723\n",
      "330 0.267881699193 0.668675\n",
      "331 0.265366943052 0.698795\n",
      "332 0.260824656687 0.686747\n",
      "333 0.267220877971 0.728916\n",
      "334 0.281733731811 0.704819\n",
      "335 0.279927913529 0.686747\n",
      "336 0.268533000579 0.680723\n",
      "337 0.265061130031 0.771084\n",
      "338 0.274393443305 0.716867\n",
      "339 0.253728859127 0.740964\n",
      "340 0.266739432772 0.662651\n",
      "341 0.242194136461 0.686747\n",
      "342 0.249040738894 0.710843\n",
      "343 0.273514197996 0.716867\n",
      "344 0.270170077108 0.73494\n",
      "345 0.271000647201 0.73494\n",
      "346 0.253626895639 0.698795\n",
      "347 0.262092841646 0.728916\n",
      "348 0.259864605963 0.753012\n",
      "349 0.263322609262 0.698795\n",
      "350 0.249165302716 0.73494\n",
      "351 0.260909124206 0.716868\n",
      "352 0.247365701371 0.728916\n",
      "353 0.282889933254 0.73494\n",
      "354 0.255118033061 0.746988\n",
      "355 0.261589877451 0.728916\n",
      "356 0.263715635412 0.668675\n",
      "357 0.245059367269 0.722892\n",
      "358 0.238607906163 0.728916\n",
      "359 0.264313065375 0.716867\n",
      "360 0.245899646041 0.728916\n",
      "361 0.27113933761 0.716867\n",
      "362 0.269438988458 0.746988\n",
      "363 0.261161305297 0.753012\n",
      "364 0.250514048223 0.686747\n",
      "365 0.251094291202 0.686747\n",
      "366 0.246118066976 0.716867\n",
      "367 0.263465466551 0.728916\n",
      "368 0.25910020591 0.740964\n",
      "369 0.251558103527 0.746988\n",
      "370 0.259785258856 0.73494\n",
      "371 0.245297460745 0.746988\n",
      "372 0.244857068675 0.753012\n",
      "373 0.248290058942 0.759036\n",
      "374 0.249102774649 0.740964\n",
      "375 0.241674006415 0.753012\n",
      "376 0.229727546541 0.76506\n",
      "377 0.265200834291 0.777108\n",
      "378 0.240820155694 0.746988\n",
      "379 0.260034268865 0.783133\n",
      "380 0.245774276411 0.753012\n",
      "381 0.240299200639 0.76506\n",
      "382 0.24363171037 0.777108\n",
      "383 0.238447792828 0.76506\n",
      "384 0.260467389455 0.783132\n",
      "385 0.247806426138 0.801205\n",
      "386 0.268569378326 0.777108\n",
      "387 0.238001442013 0.759036\n",
      "388 0.236451700043 0.759036\n",
      "389 0.238140902554 0.740964\n",
      "390 0.247235452326 0.740964\n",
      "391 0.241365541346 0.746988\n",
      "392 0.236289346877 0.728916\n",
      "393 0.250605353083 0.771084\n",
      "394 0.253402021355 0.76506\n",
      "395 0.237862413606 0.759036\n",
      "396 0.270048469305 0.76506\n",
      "397 0.250210882666 0.740964\n",
      "398 0.231379761432 0.76506\n",
      "399 0.225828500608 0.759036\n",
      "400 0.239613401345 0.759036\n",
      "401 0.23401555419 0.759036\n",
      "402 0.240069834945 0.76506\n",
      "403 0.25367407601 0.746988\n",
      "404 0.260855238025 0.73494\n",
      "405 0.235187650013 0.759036\n",
      "406 0.241192453469 0.740964\n",
      "407 0.235760797245 0.740964\n",
      "408 0.202818861374 0.746988\n",
      "409 0.220698945368 0.753012\n",
      "410 0.233750166945 0.728916\n",
      "411 0.232566174478 0.716867\n",
      "412 0.233691592916 0.73494\n",
      "413 0.2369012953 0.740964\n",
      "414 0.247025201527 0.759036\n",
      "415 0.233432016121 0.73494\n",
      "416 0.228642480305 0.771084\n",
      "417 0.236247896862 0.76506\n",
      "418 0.230803265595 0.777108\n",
      "419 0.23817658582 0.759036\n",
      "420 0.229319683921 0.789157\n",
      "421 0.223926245092 0.789157\n",
      "422 0.23752247055 0.801205\n",
      "423 0.234709917783 0.789157\n",
      "424 0.231866210986 0.783132\n",
      "425 0.228253194776 0.771084\n",
      "426 0.229801395478 0.753012\n",
      "427 0.203606366108 0.722892\n",
      "428 0.236629428199 0.753012\n",
      "429 0.2320106051 0.76506\n",
      "430 0.228345017737 0.759036\n",
      "431 0.222203942661 0.759036\n",
      "432 0.227940183706 0.783133\n",
      "433 0.230853487379 0.777108\n",
      "434 0.211144746783 0.795181\n",
      "435 0.219080342266 0.771084\n",
      "436 0.221580088425 0.76506\n",
      "437 0.224121056784 0.777108\n",
      "438 0.218826011253 0.771084\n",
      "439 0.246547647465 0.771084\n",
      "440 0.241140091792 0.777108\n",
      "441 0.237417659507 0.76506\n",
      "442 0.238796423834 0.753012\n",
      "443 0.210700433415 0.759036\n",
      "444 0.210029085668 0.740964\n",
      "445 0.223427955634 0.740964\n",
      "446 0.202222039493 0.771084\n",
      "447 0.214000742453 0.753012\n",
      "448 0.212215919191 0.777108\n",
      "449 0.220251933743 0.783132\n",
      "Checkpoint saved at  ./models/face_recog_2.ckpt\n",
      "20:22:50.877416\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 경로 설정\n",
    "save_path = \"./models/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# 세션 시작\n",
    "with tf.Session(config = tf.ConfigProto(gpu_options = tf.GPUOptions(allow_growth = True))) as sess:\n",
    "    \n",
    "    # tensorboard에 올림\n",
    "    writer = tf.summary.FileWriter('./logs/face2', sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(datetime.datetime.now().time())\n",
    "    \n",
    "    # epoch=450, batch size=128로 모델 학습\n",
    "    for i in range(450):\n",
    "        total_batch = int(len(data) / batch_size)\n",
    "        training_batch = zip(range(0, len(data), batch_size), range(batch_size, len(data)+1, batch_size))\n",
    "        li = np.zeros(1)\n",
    "        for start, end in training_batch:\n",
    "            c, _ = sess.run([cost, train_op], feed_dict = {X: data[start:end], Y: labels[start:end],\n",
    "                                           p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
    "            li[0] += c / total_batch\n",
    "        summary, acc = sess.run([merged, acc_op], feed_dict = {X: td, Y: tl, p_keep_conv: 1.0, p_keep_hidden: 1.0})\n",
    "        \n",
    "        writer.add_summary(summary, i)\n",
    "        print(i, li[0], acc)\n",
    "    \n",
    "    saved_path = saver.save(sess, os.path.join(save_path, 'face_recog_2.ckpt'))\n",
    "    print(\"Checkpoint saved at \", saved_path)\n",
    "    print(datetime.datetime.now().time())\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
