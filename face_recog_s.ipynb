{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_file_name = 'face_data.npz'\n",
    "test_file_name = 'test_data.npz'\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def init_weights(shape, name):\n",
    "    \"\"\"\n",
    "    initialize weight\n",
    "    shape: shape of weight\n",
    "    name: weight name\n",
    "    각각의 Filter를 만들어주는 함수\n",
    "    \"\"\"\n",
    "    with tf.device('/gpu:0'):\n",
    "        return tf.Variable(tf.random_normal(shape, stddev = 0.005), name = name)\n",
    "\n",
    "def init_bias(shape, name):\n",
    "    \"\"\"\n",
    "    initialize bias\n",
    "    bias를 만들어주는 함수\n",
    "    \"\"\"\n",
    "    with tf.device('/gpu:0'):\n",
    "        return tf.Variable(tf.constant(0.1, shape = shape))\n",
    "\n",
    "def model(X, w1, w2, w3, w4, wo, b2, b3, b4, bo, p_keep_conv, p_keep_hidden):\n",
    "    \"\"\"\n",
    "    create model\n",
    "    X: first input\n",
    "    w1, w2, w3, wo: weights for each layer\n",
    "    b2, b3, bo: bias for each layer\n",
    "    p_keep_conv, p_keep_hidden: constant for dropout\n",
    "    \n",
    "    모델생성 함수.\n",
    "    총 3개의 Conv layers와 3개의 Max-pool layers, 2개의 FC layers로 구성\n",
    "    Conv1 - Pooling1 - Conv2 - Pooling2 - Conv3 - Pooling3 - FC1 - FC2\n",
    "    로 구성되어있고 각 풀링 이후와 첫번째 FC 이후에 dropout을 적용\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "#         dw = tf.nn.max_pool(X, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') \n",
    "\n",
    "        # First Conv layer. l1a shape = (?, 128, 128, 32)\n",
    "        with tf.name_scope(\"layer1\"):\n",
    "            l1a = tf.nn.relu(tf.nn.conv2d(X, w1, strides = [1, 1, 1, 1], padding = 'SAME'))\n",
    "            # First Max-pool layer. l1 shape = (?, 64, 64, 32)\n",
    "            l1 = tf.nn.max_pool(l1a, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')        \n",
    "            l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "\n",
    "        with tf.name_scope(\"layer2\"):\n",
    "            # l2a shape = (?, 64, 64, 64)\n",
    "            l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, strides = [1, 1, 1, 1], padding = 'SAME') + b2)\n",
    "            # l2 shape = (?, 32, 32, 64)\n",
    "            l2 = tf.nn.max_pool(l2a, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "            l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "\n",
    "        with tf.name_scope(\"layer3\"):\n",
    "            # l2a shape = (?, 64, 64, 64)\n",
    "            l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, strides = [1, 1, 1, 1], padding = 'SAME') + b3)\n",
    "            # l2 shape = (?, 32, 32, 64)\n",
    "            l3 = tf.nn.max_pool(l3a, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "            l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])\n",
    "            l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "\n",
    "        with tf.name_scope(\"layer4\"):\n",
    "            # reshape to ( , 3072)\n",
    "            l4 = tf.nn.relu(tf.matmul(l3, w4) + b4)\n",
    "            l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "            l4 = tf.matmul(l4, wo) + bo\n",
    "            return l4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load success\n",
      "(1785, 128, 128, 1)\n",
      "(166, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(data_file_name):\n",
    "    file = np.load(data_file_name)\n",
    "    data = file['data']\n",
    "    labels = file['sex_labels']\n",
    "else:\n",
    "    print(\"No data file.\")\n",
    "    sys.exit()\n",
    "    \n",
    "if os.path.isfile(test_file_name):\n",
    "    file = np.load(test_file_name)\n",
    "    td = file['data']\n",
    "    tl = file['sex_labels']\n",
    "else:\n",
    "    print(\"No test file\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"data load success\")\n",
    "\n",
    "print(data.shape)\n",
    "print(td.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# data, labels 를 위한 placeholder\n",
    "X = tf.placeholder(\"float\", [None, 128, 128, 1], name = \"X\")\n",
    "Y = tf.placeholder(\"float\", [None, 2], name = \"Y\")\n",
    "\n",
    "\n",
    "w1 = init_weights([3, 3, 1, 32], \"weight1\") \n",
    "w2 = init_weights([3, 3, 32, 64], \"weight2\")\n",
    "w3 = init_weights([3, 3, 64, 128], \"weight3\")\n",
    "w4 = init_weights([16*16*128, 512], \"weight4\")\n",
    "wo = init_weights([512, 2], \"weight_out\")\n",
    "\n",
    "b2 = init_bias([64], \"bias2\")\n",
    "b3 = init_bias([128], \"bias3\")\n",
    "b4 = init_bias([512], \"bias4\")\n",
    "bo = init_bias([2], \"bias_out\")\n",
    "\n",
    "tf.summary.histogram(\"weight1_summ\", w1)\n",
    "tf.summary.histogram(\"weight2_summ\", w2)\n",
    "tf.summary.histogram(\"bias2_summ\", b2)\n",
    "tf.summary.histogram(\"weight3_summ\", w3)\n",
    "tf.summary.histogram(\"bias3_summ\", b3)\n",
    "tf.summary.histogram(\"weight4_summ\", w4)\n",
    "tf.summary.histogram(\"bias4_summ\", b4)\n",
    "tf.summary.histogram(\"weight_out_summ\", wo)\n",
    "tf.summary.histogram(\"bias_out_summ\", bo)\n",
    "\n",
    "# dropout 위한 placeholder\n",
    "p_keep_conv = tf.placeholder(\"float\", name = \"p_keep_conv\")\n",
    "p_keep_hidden = tf.placeholder(\"float\", name = \"p_keep_hidden\")\n",
    "\n",
    "M = model(X, w1, w2, w3, w4, wo, b2, b3, b4, bo, p_keep_conv, p_keep_hidden)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = M, labels = Y))\n",
    "    train_op = tf.train.RMSPropOptimizer(0.00001, 0.9).minimize(cost)\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"accuarcy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(M, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "    tf.summary.scalar(\"accuracy\", acc_op)\n",
    "    \n",
    "tf.add_to_collection('vars', w1)\n",
    "tf.add_to_collection('vars', w2)\n",
    "tf.add_to_collection('vars', b2)\n",
    "tf.add_to_collection('vars', w3)\n",
    "tf.add_to_collection('vars', b3)\n",
    "tf.add_to_collection('vars', w4)\n",
    "tf.add_to_collection('vars', b4)\n",
    "tf.add_to_collection('vars', wo)\n",
    "tf.add_to_collection('vars', bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:46:07.387983\n",
      "0 0.921687\n",
      "1 0.921687\n",
      "2 0.921687\n",
      "3 0.921687\n",
      "4 0.921687\n",
      "5 0.921687\n",
      "6 0.921687\n",
      "7 0.921687\n",
      "8 0.0783133\n",
      "9 0.0783132\n",
      "10 0.0783132\n",
      "11 0.0783133\n",
      "12 0.0783133\n",
      "13 0.0783133\n",
      "14 0.0783132\n",
      "15 0.0783133\n",
      "16 0.0783132\n",
      "17 0.0783132\n",
      "18 0.0783133\n",
      "19 0.0783133\n",
      "20 0.0783132\n",
      "21 0.0783133\n",
      "22 0.0783133\n",
      "23 0.0783132\n",
      "24 0.0783133\n",
      "25 0.0783132\n",
      "26 0.0783132\n",
      "27 0.0783132\n",
      "28 0.0783132\n",
      "29 0.0783133\n",
      "30 0.0783132\n",
      "31 0.0783133\n",
      "32 0.0783133\n",
      "33 0.0783133\n",
      "34 0.0783133\n",
      "35 0.0783133\n",
      "36 0.0783133\n",
      "37 0.0783133\n",
      "38 0.0783132\n",
      "39 0.0783133\n",
      "40 0.0783132\n",
      "41 0.0783132\n",
      "42 0.0783132\n",
      "43 0.0783132\n",
      "44 0.0783132\n",
      "45 0.0783132\n",
      "46 0.0783133\n",
      "47 0.0783132\n",
      "48 0.0783132\n",
      "49 0.0783132\n",
      "50 0.0783132\n",
      "51 0.0783132\n",
      "52 0.0783132\n",
      "53 0.0783133\n",
      "54 0.0783132\n",
      "55 0.0783133\n",
      "56 0.0843373\n",
      "57 0.0903614\n",
      "58 0.10241\n",
      "59 0.126506\n",
      "60 0.162651\n",
      "61 0.198795\n",
      "62 0.222892\n",
      "63 0.295181\n",
      "64 0.337349\n",
      "65 0.385542\n",
      "66 0.415663\n",
      "67 0.439759\n",
      "68 0.463855\n",
      "69 0.487952\n",
      "70 0.512048\n",
      "71 0.53012\n",
      "72 0.566265\n",
      "73 0.572289\n",
      "74 0.572289\n",
      "75 0.584337\n",
      "76 0.60241\n",
      "77 0.608434\n",
      "78 0.626506\n",
      "79 0.626506\n",
      "80 0.626506\n",
      "81 0.626506\n",
      "82 0.626506\n",
      "83 0.63253\n",
      "84 0.63253\n",
      "85 0.644578\n",
      "86 0.644578\n",
      "87 0.644578\n",
      "88 0.650602\n",
      "89 0.650602\n",
      "90 0.650602\n",
      "91 0.650602\n",
      "92 0.650602\n",
      "93 0.656626\n",
      "94 0.656626\n",
      "95 0.662651\n",
      "96 0.662651\n",
      "97 0.662651\n",
      "98 0.662651\n",
      "99 0.662651\n",
      "100 0.674699\n",
      "101 0.668675\n",
      "102 0.662651\n",
      "103 0.668675\n",
      "104 0.674699\n",
      "105 0.674699\n",
      "106 0.674699\n",
      "107 0.674699\n",
      "108 0.674699\n",
      "109 0.680723\n",
      "110 0.680723\n",
      "111 0.674699\n",
      "112 0.674699\n",
      "113 0.680723\n",
      "114 0.680723\n",
      "115 0.680723\n",
      "116 0.680723\n",
      "117 0.680723\n",
      "118 0.680723\n",
      "119 0.680723\n",
      "120 0.680723\n",
      "121 0.680723\n",
      "122 0.680723\n",
      "123 0.680723\n",
      "124 0.680723\n",
      "125 0.680723\n",
      "126 0.680723\n",
      "127 0.680723\n",
      "128 0.680723\n",
      "129 0.680723\n",
      "130 0.680723\n",
      "131 0.680723\n",
      "132 0.680723\n",
      "133 0.674699\n",
      "134 0.674699\n",
      "135 0.680723\n",
      "136 0.680723\n",
      "137 0.674699\n",
      "138 0.674699\n",
      "139 0.674699\n",
      "140 0.674699\n",
      "141 0.674699\n",
      "142 0.674699\n",
      "143 0.674699\n",
      "144 0.674699\n",
      "145 0.674699\n",
      "146 0.674699\n",
      "147 0.674699\n",
      "148 0.674699\n",
      "149 0.674699\n",
      "150 0.674699\n",
      "151 0.674699\n",
      "152 0.674699\n",
      "153 0.674699\n",
      "154 0.674699\n",
      "155 0.674699\n",
      "156 0.674699\n",
      "157 0.680723\n",
      "158 0.680723\n",
      "159 0.680723\n",
      "160 0.680723\n",
      "161 0.680723\n",
      "162 0.680723\n",
      "163 0.686747\n",
      "164 0.686747\n",
      "165 0.686747\n",
      "166 0.686747\n",
      "167 0.686747\n",
      "168 0.686747\n",
      "169 0.686747\n",
      "170 0.686747\n",
      "171 0.686747\n",
      "172 0.686747\n",
      "173 0.686747\n",
      "174 0.686747\n",
      "175 0.692771\n",
      "176 0.686747\n",
      "177 0.692771\n",
      "178 0.686747\n",
      "179 0.692771\n",
      "180 0.698795\n",
      "181 0.698795\n",
      "182 0.686747\n",
      "183 0.698795\n",
      "184 0.698795\n",
      "185 0.698795\n",
      "186 0.698795\n",
      "187 0.698795\n",
      "188 0.704819\n",
      "189 0.692771\n",
      "190 0.704819\n",
      "191 0.704819\n",
      "192 0.692771\n",
      "193 0.692771\n",
      "194 0.692771\n",
      "195 0.698795\n",
      "196 0.698795\n",
      "197 0.692771\n",
      "198 0.704819\n",
      "199 0.698795\n",
      "200 0.698795\n",
      "201 0.698795\n",
      "202 0.698795\n",
      "203 0.698795\n",
      "204 0.692771\n",
      "205 0.692771\n",
      "206 0.698795\n",
      "207 0.692771\n",
      "208 0.692771\n",
      "209 0.698795\n",
      "210 0.698795\n",
      "211 0.692771\n",
      "212 0.698795\n",
      "213 0.698795\n",
      "214 0.698795\n",
      "215 0.698795\n",
      "216 0.698795\n",
      "217 0.698795\n",
      "218 0.698795\n",
      "219 0.698795\n",
      "220 0.698795\n",
      "221 0.698795\n",
      "222 0.692771\n",
      "223 0.692771\n",
      "224 0.698795\n",
      "225 0.692771\n",
      "226 0.692771\n",
      "227 0.692771\n",
      "228 0.692771\n",
      "229 0.692771\n",
      "230 0.692771\n",
      "231 0.692771\n",
      "232 0.692771\n",
      "233 0.692771\n",
      "234 0.692771\n",
      "235 0.692771\n",
      "236 0.692771\n",
      "237 0.692771\n",
      "238 0.698795\n",
      "239 0.698795\n",
      "240 0.698795\n",
      "241 0.698795\n",
      "242 0.704819\n",
      "243 0.698795\n",
      "244 0.698795\n",
      "245 0.704819\n",
      "246 0.704819\n",
      "247 0.704819\n",
      "248 0.704819\n",
      "249 0.704819\n",
      "250 0.698795\n",
      "251 0.698795\n",
      "252 0.698795\n",
      "253 0.698795\n",
      "254 0.698795\n",
      "255 0.710843\n",
      "256 0.704819\n",
      "257 0.710843\n",
      "258 0.710843\n",
      "259 0.704819\n",
      "260 0.710843\n",
      "261 0.710843\n",
      "262 0.710843\n",
      "263 0.710843\n",
      "264 0.710843\n",
      "265 0.710843\n",
      "266 0.710843\n",
      "267 0.716867\n",
      "268 0.710843\n",
      "269 0.710843\n",
      "270 0.716867\n",
      "271 0.710843\n",
      "272 0.716867\n",
      "273 0.716867\n",
      "274 0.716867\n",
      "275 0.716867\n",
      "276 0.710843\n",
      "277 0.710843\n",
      "278 0.716867\n",
      "279 0.710843\n",
      "280 0.710843\n",
      "281 0.716867\n",
      "282 0.710843\n",
      "283 0.710843\n",
      "284 0.710843\n",
      "285 0.710843\n",
      "286 0.710843\n",
      "287 0.710843\n",
      "288 0.710843\n",
      "289 0.710843\n",
      "290 0.710843\n",
      "291 0.710843\n",
      "292 0.710843\n",
      "293 0.710843\n",
      "294 0.710843\n",
      "295 0.716867\n",
      "296 0.716867\n",
      "297 0.716867\n",
      "298 0.722892\n",
      "299 0.722892\n",
      "300 0.710843\n",
      "301 0.722892\n",
      "302 0.722892\n",
      "303 0.722892\n",
      "304 0.722892\n",
      "305 0.722892\n",
      "306 0.728916\n",
      "307 0.728916\n",
      "308 0.73494\n",
      "309 0.73494\n",
      "310 0.73494\n",
      "311 0.73494\n",
      "312 0.73494\n",
      "313 0.728916\n",
      "314 0.73494\n",
      "315 0.73494\n",
      "316 0.73494\n",
      "317 0.728916\n",
      "318 0.73494\n",
      "319 0.73494\n",
      "320 0.728916\n",
      "321 0.728916\n",
      "322 0.728916\n",
      "323 0.73494\n",
      "324 0.740964\n",
      "325 0.740964\n",
      "326 0.73494\n",
      "327 0.73494\n",
      "328 0.73494\n",
      "329 0.73494\n",
      "330 0.73494\n",
      "331 0.73494\n",
      "332 0.73494\n",
      "333 0.73494\n",
      "334 0.73494\n",
      "335 0.73494\n",
      "336 0.740964\n",
      "337 0.740964\n",
      "338 0.740964\n",
      "339 0.740964\n",
      "340 0.740964\n",
      "341 0.740964\n",
      "342 0.740964\n",
      "343 0.740964\n",
      "344 0.740964\n",
      "345 0.740964\n",
      "346 0.740964\n",
      "347 0.746988\n",
      "348 0.746988\n",
      "349 0.740964\n",
      "350 0.740964\n",
      "351 0.753012\n",
      "352 0.746988\n",
      "353 0.753012\n",
      "354 0.753012\n",
      "355 0.753012\n",
      "356 0.753012\n",
      "357 0.753012\n",
      "358 0.753012\n",
      "359 0.753012\n",
      "360 0.753012\n",
      "361 0.753012\n",
      "362 0.759036\n",
      "363 0.753012\n",
      "364 0.753012\n",
      "365 0.76506\n",
      "366 0.76506\n",
      "367 0.76506\n",
      "368 0.759036\n",
      "369 0.771084\n",
      "370 0.76506\n",
      "371 0.76506\n",
      "372 0.76506\n",
      "373 0.76506\n",
      "374 0.76506\n",
      "375 0.771084\n",
      "376 0.76506\n",
      "377 0.771084\n",
      "378 0.771084\n",
      "379 0.771084\n",
      "380 0.771084\n",
      "381 0.771084\n",
      "382 0.771084\n",
      "383 0.771084\n",
      "384 0.771084\n",
      "385 0.771084\n",
      "386 0.771084\n",
      "387 0.771084\n",
      "388 0.771084\n",
      "389 0.771084\n",
      "390 0.771084\n",
      "391 0.771084\n",
      "392 0.771084\n",
      "393 0.771084\n",
      "394 0.771084\n",
      "395 0.777108\n",
      "396 0.777108\n",
      "397 0.777108\n",
      "398 0.777108\n",
      "399 0.777108\n",
      "400 0.777108\n",
      "401 0.777108\n",
      "402 0.777108\n",
      "403 0.777108\n",
      "404 0.777108\n",
      "405 0.783132\n",
      "406 0.777108\n",
      "407 0.777108\n",
      "408 0.777108\n",
      "409 0.777108\n",
      "410 0.777108\n",
      "411 0.777108\n",
      "412 0.777108\n",
      "413 0.777108\n",
      "414 0.777108\n",
      "415 0.777108\n",
      "416 0.777108\n",
      "417 0.777108\n",
      "418 0.777108\n",
      "419 0.789157\n",
      "420 0.777108\n",
      "421 0.783132\n",
      "422 0.783133\n",
      "423 0.789157\n",
      "424 0.783132\n",
      "425 0.783132\n",
      "426 0.777108\n",
      "427 0.783132\n",
      "428 0.783133\n",
      "429 0.783132\n",
      "430 0.783132\n",
      "431 0.783132\n",
      "432 0.783132\n",
      "433 0.789157\n",
      "434 0.783133\n",
      "435 0.783133\n",
      "436 0.783132\n",
      "437 0.783133\n",
      "438 0.783133\n",
      "439 0.783132\n",
      "440 0.783132\n",
      "441 0.783132\n",
      "442 0.795181\n",
      "443 0.777108\n",
      "444 0.783132\n",
      "445 0.783132\n",
      "446 0.783133\n",
      "447 0.783132\n",
      "448 0.783132\n",
      "449 0.783132\n",
      "Checkpoint saved at  ./models/face_recog_2.ckpt\n",
      "15:03:42.532334\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./models/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "with tf.Session(config = tf.ConfigProto(gpu_options = tf.GPUOptions(allow_growth = True))) as sess:\n",
    "    \n",
    "    writer = tf.summary.FileWriter('./logs/face2', sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(datetime.datetime.now().time())\n",
    "    \n",
    "    for i in range(450):\n",
    "        training_batch = zip(range(0, len(data), batch_size), range(batch_size, len(data)+1, batch_size))\n",
    "        for start, end in training_batch:\n",
    "            sess.run(train_op, feed_dict = {X: data[start:end], Y: labels[start:end],\n",
    "                                           p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
    "        \n",
    "        summary, acc = sess.run([merged, acc_op], feed_dict = {X: td, Y: tl, p_keep_conv: 1.0, p_keep_hidden: 1.0})\n",
    "        \n",
    "        writer.add_summary(summary, i)\n",
    "        print(i, acc)\n",
    "    \n",
    "    saved_path = saver.save(sess, os.path.join(save_path, 'face_recog_2.ckpt'))\n",
    "    print(\"Checkpoint saved at \", saved_path)\n",
    "    print(datetime.datetime.now().time())\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
